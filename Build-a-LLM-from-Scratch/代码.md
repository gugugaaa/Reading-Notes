å„ä¸ªç« èŠ‚çš„å…³é”®ä»£ç 
===

ç¬¬äºŒç« 
---
å¤„ç†æ–‡æœ¬æ•°æ®

### æ•°æ®é›†ç±» GPTDatasetV1
```python
class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        # åˆå§‹åŒ–è¾“å…¥-ç›®æ ‡å¯¹
        self.input_ids = []
        self.target_ids = []

        # åˆ†è¯åŒ–æ–‡æœ¬ï¼ˆåˆ©ç”¨ä¼ å‚çš„åˆ†è¯å™¨ï¼‰
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # ç”¨æ»‘åŠ¨çª—å£åˆ›å»ºè¾“å…¥-ç›®æ ‡å¯¹
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            # å¡«å……è¾“å…¥å—å’Œç›®æ ‡å—
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]
```

### æ•°æ®åŠ è½½å™¨å‡½æ•° create_dataloader_v1
```python
def create_dataloader_v1(txt, batch_size, max_length, stride,
                         shuffle=True, drop_last=True, num_workers=0):
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = tiktoken.get_encoding("gpt2")

    # åˆ›å»ºæ•°æ®é›†
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader
```

ç¬¬ä¸‰ç« 
---
å®ç°æ³¨æ„åŠ›æœºåˆ¶

### å¯è®­ç»ƒå‚æ•°ï¼Œå•å±‚æ³¨æ„åŠ›ï¼Œæ— å› æœæ©ç 
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp" width="400px">

```python
class SelfAttention_v2(nn.Module):

    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        # åˆ›å»ºä¸‰ä¸ªå¯è®­ç»ƒå‚æ•° W_query, W_key, W_value
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = queries @ keys.T
        # Softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ Z
        context_vec = attn_weights @ values
        return context_vec
```

### æœ‰å› æœæ©ç ï¼Œå¤šå¤´æ³¨æ„åŠ›
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp" width="400px">

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert (d_out % num_heads == 0), \
            "è¾“å‡ºç»´åº¦å¿…é¡»èƒ½è¢«æ³¨æ„åŠ›å¤´æ•°æ•´é™¤"

        self.d_out = d_out              # è¾“å‡ºç»´åº¦
        self.num_heads = num_heads      # æ³¨æ„åŠ›å¤´æ•°
        self.head_dim = d_out // num_heads  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦

        # ç”¨äºè®¡ç®—æŸ¥è¯¢(Q)ã€é”®(K)å’Œå€¼(V)çš„çº¿æ€§å˜æ¢å±‚
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # åˆå¹¶å¤šå¤´è¾“å‡ºçš„çº¿æ€§å±‚
        self.dropout = nn.Dropout(dropout)
        
        # æ³¨å†Œå› æœæ©ç ï¼ˆç¡®ä¿æ¨¡å‹åªèƒ½çœ‹åˆ°å½“å‰åŠä¹‹å‰çš„tokenï¼‰
        self.register_buffer(
            "mask",
            torch.triu(torch.ones(context_length, context_length),
                       diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape   # æ‰¹æ¬¡å¤§å°ã€åºåˆ—é•¿åº¦ã€è¾“å…¥ç»´åº¦

        # è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µ
        keys = self.W_key(x)            # å½¢çŠ¶: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # é€šè¿‡æ·»åŠ num_headsç»´åº¦æ¥éšå¼åˆ†å‰²çŸ©é˜µ
        # é‡å¡‘ç»´åº¦: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) 
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # è½¬ç½®: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # ä½¿ç”¨å› æœæ©ç è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        attn_scores = queries @ keys.transpose(2, 3)  # è®¡ç®—æ¯ä¸ªå¤´çš„ç‚¹ç§¯æ³¨æ„åŠ›åˆ†æ•°

        # å°†åŸå§‹æ©ç æˆªæ–­åˆ°å®é™…tokenæ•°é‡å¹¶è½¬æ¢ä¸ºå¸ƒå°”å€¼
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # ä½¿ç”¨æ©ç å¡«å……æ³¨æ„åŠ›åˆ†æ•°
        attn_scores.masked_fill_(mask_bool, -torch.inf)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶åº”ç”¨dropout
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ å½¢çŠ¶: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2) 
        
        # åˆå¹¶æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„è¾“å‡ºï¼Œå…¶ä¸­ self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # å¯é€‰çš„è¾“å‡ºæŠ•å½±

        return context_vec
```

> ä»€ä¹ˆæ˜¯ bufferï¼Ÿ
> 
> - *Parameters: éœ€è¦æ¢¯åº¦ã€ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°*
> - Buffers: ä¸éœ€è¦æ¢¯åº¦ã€ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°
>
> Buffers æ˜¯**æ¨¡å‹ä¸­ä¸éœ€è¦è®¡ç®—æ¢¯åº¦çš„å¼ é‡å‚æ•°**ï¼Œé€šå¸¸æŠŠ*æ³¨æ„åŠ›æ©ç ï¼Œbatch norm å‚æ•°ç­‰* æ³¨å†Œä¸º bufferã€‚

ç¬¬å››ç« 
---
å®ç°GPTæ¥ç”Ÿæˆæ–‡æœ¬

### å®šä¹‰æ¨¡å‹å‚æ•°
```python
GPT_CONFIG_124M = {
        "vocab_size": 50257,     # è¯æ±‡è¡¨å¤§å° 
        "context_length": 1024,  # ä¸Šä¸‹æ–‡é•¿åº¦
        "emb_dim": 768,         # åµŒå…¥ç»´åº¦
        "n_heads": 12,          # æ³¨æ„åŠ›å¤´æ•°é‡
        "n_layers": 12,         # å±‚æ•°
        "drop_rate": 0.1,       # Dropoutç‡
        "qkv_bias": False       # æŸ¥è¯¢-é”®-å€¼åç½®é¡¹
    }
```

### GPTå…¶ä»–éƒ¨åˆ†çš„å®ç°

#### å±‚å½’ä¸€åŒ–
```python
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift
```

#### æ¿€æ´»å‡½æ•°ï¼ˆGELUï¼‰
```python
class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))
```

#### å‰é¦ˆç½‘ç»œ
```python
class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)
```

### å°è£… Transformer å±‚
![alt text](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1)


```python
class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        # å¤šå¤´æ³¨æ„åŠ›
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        # å‰é¦ˆç½‘ç»œ
        self.ff = FeedForward(cfg)
        # å±‚å½’ä¸€åŒ–
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        # Dropout
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x
```

### GPTæ¨¡å‹
![alt text](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp)

```python
class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        # è¯åµŒå…¥
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        # ä½ç½®åµŒå…¥
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        # Dropout
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        # Transformer å±‚
        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])
        # å±‚å½’ä¸€åŒ–
        self.final_norm = LayerNorm(cfg["emb_dim"])
        # è¾“å‡ºå±‚
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
```

ç¬¬äº”ç« 
---
åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šé¢„è®­ç»ƒ

### ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹

```mermaid
graph TD
    A[å¼€å§‹] --> B[ä¸‹è½½/åŠ è½½æ•°æ®]
    B --> C[åˆå§‹åŒ–GPTæ¨¡å‹]
    C --> D[åˆ›å»ºä¼˜åŒ–å™¨AdamW]
    D --> E[åˆ†å‰²è®­ç»ƒ/éªŒè¯æ•°æ®]
    E --> F[åˆ›å»ºæ•°æ®åŠ è½½å™¨]
    F --> G[åˆå§‹åŒ–tokenizer]
    
    G --> H[å¼€å§‹è®­ç»ƒå¾ªç¯]
    H --> I{epoch < max_epochs?}
    I -->|æ˜¯| J[è®¡ç®—batchæŸå¤±]
    J --> K[åå‘ä¼ æ’­]
    K --> L[æ›´æ–°æ¨¡å‹å‚æ•°]
    L --> M{æ˜¯å¦è¯„ä¼°?}
    M -->|æ˜¯| N[è®¡ç®—è®­ç»ƒ/éªŒè¯æŸå¤±]
    N --> O[ç”Ÿæˆæ ·æœ¬æ–‡æœ¬]
    O --> I
    M -->|å¦| J
    I -->|å¦| P[è®­ç»ƒç»“æŸ]
    
    P --> Q[ç»˜åˆ¶æŸå¤±æ›²çº¿]
    Q --> R[ä¿å­˜æ¨¡å‹]
    R --> S[ç»“æŸ]
```

#### è®¡ç®—æŸå¤±å‡½æ•°
```python
def calc_loss_loader(data_loader, model, device, num_batches=None):

    total_loss = 0.  # åˆå§‹åŒ–æ€»æŸå¤±
    
    # å¦‚æœæ•°æ®åŠ è½½å™¨ä¸ºç©º,è¿”å›NaN
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)  # å¦‚æœæœªæŒ‡å®šæ‰¹æ¬¡æ•°,ä½¿ç”¨å…¨éƒ¨æ‰¹æ¬¡
    else:
        # å¦‚æœæŒ‡å®šçš„æ‰¹æ¬¡æ•°è¶…è¿‡æ•°æ®åŠ è½½å™¨çš„æ‰¹æ¬¡æ€»æ•°,åˆ™ä½¿ç”¨å®é™…çš„æ‰¹æ¬¡æ€»æ•°
        num_batches = min(num_batches, len(data_loader))
    
    # éå†æ•°æ®åŠ è½½å™¨ä¸­çš„æ‰¹æ¬¡
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æŸå¤±å¹¶ç´¯åŠ 
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
            
    # è¿”å›å¹³å‡æŸå¤±
    return total_loss / num_batches
```

#### åœ¨æœ‰ç›‘ç£æ•°æ®ä¸Šè®­ç»ƒ
```python
def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter):
    # åˆå§‹åŒ–åˆ—è¡¨ä»¥è·Ÿè¸ªæŸå¤±å’Œå·²å¤„ç†æ ·æœ¬æ•°
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad() # é‡ç½®æ¢¯åº¦
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward() # è®¡ç®—æ¢¯åº¦
            optimizer.step() # æ›´æ–°å‚æ•°
            examples_seen += input_batch.shape[0] # è®°å½•å·²å¤„ç†æ ·æœ¬æ•°ï¼ˆç”»å›¾ç”¨ï¼‰
            global_step += 1

            # æ¯ eval_freq æ­¥è¯„ä¼°æ¨¡å‹
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # æ¯ä¸ªepochç»“æŸåè¯„ä¼°æ¨¡å‹
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen
```

### ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨OpenAIçš„GPT-2æ¨¡å‹ï¼‰
```mermaid
graph TD
    E[è¾“å…¥æ–‡æœ¬] --> F[text_to_token_ids]
    F --> G[generateå‡½æ•°]
    G --> G1[è·å–æ¡ä»¶ç´¢å¼•]
    G1 --> G2[è®¡ç®—logits]  
    G2 --> G3{ä½¿ç”¨top_k?}
    G3 -->|æ˜¯| G4[åº”ç”¨top_kè¿‡æ»¤]
    G3 -->|å¦| G5[ç›´æ¥ä½¿ç”¨logits]
    G4 --> G6{ä½¿ç”¨temperature?}
    G5 --> G6
    G6 -->|æ˜¯| G7[åº”ç”¨temperatureé‡‡æ ·]
    G6 -->|å¦| G8[å–æœ€å¤§å€¼]
    G7 --> G9[æ‹¼æ¥æ–°token]
    G8 --> G9
    G9 --> G10{è¾¾åˆ°æœ€å¤§é•¿åº¦?}
    G10 -->|å¦| G1
    G10 -->|æ˜¯| H[token_ids_to_text]
    H --> I[è¾“å‡ºç”Ÿæˆæ–‡æœ¬]
```

#### æ–‡æœ¬ $ \leftrightarrow $ token_id è½¬æ¢å‡½æ•°
```python
def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())
```

#### ç”Ÿæˆæ–‡æœ¬å‡½æ•°
```python
def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # ä½¿ç”¨å¾ªç¯éå†ï¼šè·å–logitsï¼Œåªå…³æ³¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # æ–°å¢ï¼šä½¿ç”¨top_ké‡‡æ ·è¿‡æ»¤logits
        if top_k is not None:
            # åªä¿ç•™top_kä¸ªå€¼
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

        # æ–°å¢ï¼šåº”ç”¨æ¸©åº¦ç¼©æ”¾
        if temperature > 0.0:
            logits = logits / temperature

            # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # ä»åˆ†å¸ƒä¸­é‡‡æ ·
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # å¦åˆ™ä¸ä¹‹å‰ç›¸åŒï¼šè·å–å…·æœ‰æœ€é«˜logitså€¼çš„è¯æ±‡è¡¨æ¡ç›®çš„ç´¢å¼•
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        # å¦‚æœé‡åˆ°ç»“æŸåºåˆ—æ ‡è®°ä¸”æŒ‡å®šäº†eos_idï¼Œåˆ™æå‰åœæ­¢ç”Ÿæˆ
        if idx_next == eos_id:
            break

        # ä¸ä¹‹å‰ç›¸åŒï¼šå°†é‡‡æ ·çš„ç´¢å¼•é™„åŠ åˆ°è¿è¡Œåºåˆ—ä¸­
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
```

ç¬¬å…­ç« 
---
åˆ†ç±»ä»»åŠ¡å¾®è°ƒ

### æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
![alt text](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/pad-input-sequences.webp?123)


```mermaid
flowchart TD
    A[CSVæ–‡ä»¶è¾“å…¥] --> B[è¯»å–æ•°æ®]
    B --> C[æ–‡æœ¬tokenization]
    C --> D{è®¾ç½®äº†max_length?}
    D -->|æ˜¯| E[æˆªæ–­åºåˆ—]
    D -->|å¦| F[è®¡ç®—æœ€é•¿åºåˆ—é•¿åº¦]
    E --> G[åºåˆ—å¡«å……]
    F --> G
    G --> H[è¿”å›tensoræ ¼å¼æ•°æ®]
```

> å¤ç”¨ä¹‹å‰çš„æ•°æ®åŠ è½½å™¨ï¼Œåˆ¶ä½œ`train_loader` å’Œ `valid_loader`

### æ„å»ºåˆ†ç±»å™¨

![alt text](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/lm-head.webp)

```python
torch.manual_seed(123)

num_classes = 2
model.out_head = torch.nn.Linear(in_features=BASE_CONFIG["emb_dim"], out_features=num_classes)
```

### æŠŠæ¨¡å‹ç”¨äºåƒåœ¾é‚®ä»¶åˆ†ç±»

```python
def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):
    model.eval()

    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    input_ids = tokenizer.encode(text)
    supported_context_length = model.pos_emb.weight.shape[0]

    # æˆªæ–­åºåˆ—ï¼ˆå¦‚æœå¤ªé•¿ï¼‰
    input_ids = input_ids[:min(max_length, supported_context_length)]

    # å¡«å……åºåˆ—
    input_ids += [pad_token_id] * (max_length - len(input_ids))
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension

    # ç”Ÿæˆæ¨¡å‹è¾“å‡º
    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token
    predicted_label = torch.argmax(logits, dim=-1).item()

    # è¿”å›é¢„æµ‹æ ‡ç­¾
    return "spam" if predicted_label == 1 else "not spam"
```

ç¬¬ä¸ƒç« 
---
æŒ‡ä»¤å¾®è°ƒ

### æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
#### è½¬æ¢ä¸ºPrompt Style
```python
def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text
```

#### åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†
```python
class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # åˆå§‹åŒ–ç¼–ç æ–‡æœ¬åˆ—è¡¨
        self.encoded_texts = []
        # éå†æ•°æ®å¹¶ç¼–ç 
        for entry in data:
            # æ ¼å¼åŒ–è¾“å…¥
            instruction_plus_input = format_input(entry)
            # æ ¼å¼åŒ–è¾“å‡º
            response_text = f"\n\n### Response:\n{entry['output']}"
            # åˆå¹¶æ–‡æœ¬
            full_text = instruction_plus_input + response_text
            # ç¼–ç æ–‡æœ¬
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)
```

#### åˆ¶ä½œè¾“å…¥-ç›®æ ‡å¯¹
```mermaid
flowchart TD
    A[å¼€å§‹] --> B[è®¡ç®—æ‰¹æ¬¡æœ€å¤§é•¿åº¦]
    B --> C[å¾ªç¯å¤„ç†æ¯ä¸ªåºåˆ—]
    C --> D[æ·»åŠ ç»“æŸç¬¦å·]
    D --> E[åºåˆ—å¡«å……]
    E --> F[å‡†å¤‡è¾“å…¥åºåˆ—<br>æˆªæ–­æœ€åä¸€ä¸ªtoken]
    E --> G[å‡†å¤‡ç›®æ ‡åºåˆ—<br>å³ç§»ä¸€ä½]
    
    F --> H[å¤„ç†å¡«å……æ ‡è®°]
    G --> H
    
    H --> I{æ˜¯å¦éœ€è¦æˆªæ–­?}
    I -->|æ˜¯| J[æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦]
    I -->|å¦| K[æ·»åŠ åˆ°åˆ—è¡¨]
    J --> K
    
    K --> L[æ˜¯å¦å¤„ç†å®Œæ‰€æœ‰åºåˆ—?]
    L -->|å¦| C
    L -->|æ˜¯| M[è½¬æ¢ä¸ºå¼ é‡<br>ç§»è‡³æŒ‡å®šè®¾å¤‡]
    M --> N[è¿”å›è¾“å…¥å’Œç›®æ ‡å¼ é‡]
```

> åŒ†å¿™ç»“æŸ...
> å‰©ä¸‹çš„éƒ½å·®ä¸å¤šäº†
> è‡ªåŠ¨ä¼šè¯åŸºå‡†è¯„ä¼°æˆ‘ä¹Ÿæ™•æ™•çš„å°±ä¸çœ‹äº†ğŸ˜Š